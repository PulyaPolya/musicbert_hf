{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d67aaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ui556004/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now using: /rwthfs/rz/cluster/home/ui556004/projects/musicbert_hf/musicbert_hf/__init__.py\n",
      "HDF5Dataset from: /rwthfs/rz/cluster/home/ui556004/projects/musicbert_hf/musicbert_hf/data.py\n",
      "Now using: /rwthfs/rz/cluster/home/ui556004/projects/musicbert_hf/musicbert_hf/__init__.py\n",
      "Package directory: /rwthfs/rz/cluster/home/ui556004/projects/musicbert_hf/musicbert_hf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, os, importlib\n",
    "\n",
    "# This is the repo root that contains the \"musicbert_hf\" folder\n",
    "project_root = \"/rwthfs/rz/cluster/home/ui556004/projects/musicbert_hf\"\n",
    "\n",
    "# Make sure it's first on sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Drop already-loaded venv version of musicbert_hf, if any\n",
    "if \"musicbert_hf\" in sys.modules:\n",
    "    del sys.modules[\"musicbert_hf\"]\n",
    "\n",
    "import musicbert_hf\n",
    "for name in list(sys.modules.keys()):\n",
    "    if name.startswith(\"musicbert_hf\"):\n",
    "        del sys.modules[name]\n",
    "\n",
    "import musicbert_hf\n",
    "from musicbert_hf.data import HDF5Dataset\n",
    "import inspect\n",
    "\n",
    "print(\"Now using:\", musicbert_hf.__file__)\n",
    "print(\"HDF5Dataset from:\", inspect.getsourcefile(HDF5Dataset))\n",
    "print(\"Now using:\", musicbert_hf.__file__)\n",
    "print(\"Package directory:\", os.path.dirname(musicbert_hf.__file__))\n",
    "import sys\n",
    "import os\n",
    "from functools import partial\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "\n",
    "#sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "from musicbert_hf.data import HDF5Dataset, collate_for_musicbert_fn\n",
    "from musicbert_hf.metrics import compute_metrics, compute_metrics_multitask\n",
    "from helpers import set_seed, load_baseline_params, get_dataset, create_hyperparams_dict, load_model\n",
    "from config import load_config\n",
    "from musicbert_hf.models import freeze_layers, MusicBertTokenClassification, MusicBertMultiTaskTokenClassification, MusicBertMultiTaskTokenClassConditioned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39274b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now using: /rwthfs/rz/cluster/home/ui556004/projects/musicbert_hf/musicbert_hf/__init__.py\n",
      "HDF5Dataset from: /rwthfs/rz/cluster/home/ui556004/projects/musicbert_hf/musicbert_hf/data.py\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "project_root = \"/rwthfs/rz/cluster/home/ui556004/projects/musicbert_hf\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Remove *all* musicbert_hf-related modules\n",
    "for name in list(sys.modules.keys()):\n",
    "    if name.startswith(\"musicbert_hf\"):\n",
    "        del sys.modules[name]\n",
    "\n",
    "import musicbert_hf\n",
    "from musicbert_hf.data import HDF5Dataset\n",
    "import inspect\n",
    "\n",
    "print(\"Now using:\", musicbert_hf.__file__)\n",
    "print(\"HDF5Dataset from:\", inspect.getsourcefile(HDF5Dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38a1bfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusicBERT package: /rwthfs/rz/cluster/home/ui556004/projects/musicbert_hf/musicbert_hf/__init__.py\n",
      "HDF5Dataset defined in: /rwthfs/rz/cluster/home/ui556004/projects/musicbert_hf/.venv/src/musicbert-hf/musicbert_hf/data.py\n"
     ]
    }
   ],
   "source": [
    "import musicbert_hf\n",
    "import inspect\n",
    "from musicbert_hf.data import HDF5Dataset\n",
    "\n",
    "print(\"MusicBERT package:\", musicbert_hf.__file__)\n",
    "print(\"HDF5Dataset defined in:\", inspect.getsourcefile(HDF5Dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e4a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8065e1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating the model from hpo\n",
      "Head for quality\n",
      "RobertaSequenceTaggingHead(\n",
      "  (input_dropout): Dropout(p=0, inplace=False)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=732, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.09983689107917987, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=732, out_features=571, bias=True)\n",
      "      (1): LayerNorm((571,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.2571172192068058, inplace=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=571, out_features=473, bias=True)\n",
      "      (1): Identity()\n",
      "      (2): Tanh()\n",
      "      (3): Dropout(p=0.29620728443102123, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (out_proj): Linear(in_features=473, out_features=15, bias=True)\n",
      ")\n",
      "Head for inversion\n",
      "RobertaSequenceTaggingHead(\n",
      "  (input_dropout): Dropout(p=0, inplace=False)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=520, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.3861223846483287, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=520, out_features=261, bias=True)\n",
      "      (1): Identity()\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.0993578407670862, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (out_proj): Linear(in_features=261, out_features=8, bias=True)\n",
      ")\n",
      "Head for key_pc_mode\n",
      "RobertaSequenceTaggingHead(\n",
      "  (input_dropout): Dropout(p=0, inplace=False)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=380, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.31670187825521173, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=380, out_features=120, bias=True)\n",
      "      (1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.43573029509385885, inplace=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=120, out_features=557, bias=True)\n",
      "      (1): Identity()\n",
      "      (2): Tanh()\n",
      "      (3): Dropout(p=0.40183603844955723, inplace=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=557, out_features=592, bias=True)\n",
      "      (1): LayerNorm((592,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): Tanh()\n",
      "      (3): Dropout(p=0.09328502944301792, inplace=False)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=592, out_features=445, bias=True)\n",
      "      (1): LayerNorm((445,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): Tanh()\n",
      "      (3): Dropout(p=0.4462794992449889, inplace=False)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Linear(in_features=445, out_features=600, bias=True)\n",
      "      (1): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Dropout(p=0.26967112095782536, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (out_proj): Linear(in_features=600, out_features=28, bias=True)\n",
      ")\n",
      "Head for primary_alteration_primary_degree_secondary_alteration_secondary_degree\n",
      "RobertaSequenceTaggingHead(\n",
      "  (input_dropout): Dropout(p=0, inplace=False)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=280, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.11881877199619983, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (out_proj): Linear(in_features=280, out_features=185, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "args =load_config(\"test_params.yaml\")\n",
    "\n",
    "model, config = load_model(args)\n",
    "model.config.targets = list(config.targets)\n",
    "\n",
    "test_args = TrainingArguments(\n",
    "#output_dir=best_model_dir,\n",
    "per_device_eval_batch_size=args.batch_size,\n",
    "report_to=None,\n",
    "do_train=False,\n",
    "do_eval=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c5e2c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading to device cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def _get_dataset(config, split):\n",
    "    data_dir = getattr(config, f\"{split}_dir\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"loading to device {device}\")\n",
    "    dataset = HDF5Dataset(\n",
    "        os.path.join(data_dir, \"events.h5\"),\n",
    "        config.target_paths(split),\n",
    "        conditioning_path=config.conditioning_path(split),\n",
    "        device = device\n",
    "    )\n",
    "    return dataset\n",
    "compute_metrics_fn = partial(\n",
    "compute_metrics_multitask, task_names=args.targets\n",
    ") if args.multitask else compute_metrics\n",
    "\n",
    "test_dataset = _get_dataset(args, \"test\")\n",
    "test_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=test_args,\n",
    "    data_collator=partial(collate_for_musicbert_fn, multitask=args.multitask),\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d0e22db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FrozenTrial(number=0, state=1, values=[0.8286995266022146], datetime_start=datetime.datetime(2026, 1, 4, 14, 27, 24, 797362), datetime_complete=datetime.datetime(2026, 1, 4, 14, 47, 34, 78001), params={'num_linear_layers_quality': 3, 'layer_dim_quality_0': 732, 'layer_dim_quality_1': 571, 'layer_dim_quality_2': 473, 'layer_dim_quality_3': 146, 'layer_dim_quality_4': 146, 'layer_dim_quality_5': 74, 'activation_fn_quality_0': 'tanh', 'activation_fn_quality_1': 'relu', 'activation_fn_quality_2': 'tanh', 'activation_fn_quality_3': 'relu', 'activation_fn_quality_4': 'relu', 'activation_fn_quality_5': 'gelu', 'input_dropout_quality': 0.3925879806965068, 'pooler_dropout_quality_0': 0.09983689107917987, 'pooler_dropout_quality_1': 0.2571172192068058, 'pooler_dropout_quality_2': 0.29620728443102123, 'pooler_dropout_quality_3': 0.023225206359998862, 'pooler_dropout_quality_4': 0.3037724259507192, 'pooler_dropout_quality_5': 0.08526206184364576, 'normalisation_quality_0': 'layer', 'normalisation_quality_1': 'none', 'normalisation_quality_2': 'none', 'normalisation_quality_3': 'none', 'normalisation_quality_4': 'layer', 'normalisation_quality_5': 'layer', 'num_linear_layers_inversion': 2, 'layer_dim_inversion_0': 520, 'layer_dim_inversion_1': 261, 'layer_dim_inversion_2': 415, 'layer_dim_inversion_3': 434, 'layer_dim_inversion_4': 168, 'layer_dim_inversion_5': 746, 'activation_fn_inversion_0': 'relu', 'activation_fn_inversion_1': 'relu', 'activation_fn_inversion_2': 'gelu', 'activation_fn_inversion_3': 'gelu', 'activation_fn_inversion_4': 'gelu', 'activation_fn_inversion_5': 'relu', 'input_dropout_inversion': 0.49344346830025865, 'pooler_dropout_inversion_0': 0.3861223846483287, 'pooler_dropout_inversion_1': 0.0993578407670862, 'pooler_dropout_inversion_2': 0.0027610585618011996, 'pooler_dropout_inversion_3': 0.4077307142274171, 'pooler_dropout_inversion_4': 0.35342867192380856, 'pooler_dropout_inversion_5': 0.36450358402049365, 'normalisation_inversion_0': 'none', 'normalisation_inversion_1': 'none', 'normalisation_inversion_2': 'none', 'normalisation_inversion_3': 'none', 'normalisation_inversion_4': 'layer', 'normalisation_inversion_5': 'none', 'num_linear_layers_key_pc_mode': 6, 'layer_dim_key_pc_mode_0': 380, 'layer_dim_key_pc_mode_1': 120, 'layer_dim_key_pc_mode_2': 557, 'layer_dim_key_pc_mode_3': 592, 'layer_dim_key_pc_mode_4': 445, 'layer_dim_key_pc_mode_5': 600, 'activation_fn_key_pc_mode_0': 'relu', 'activation_fn_key_pc_mode_1': 'relu', 'activation_fn_key_pc_mode_2': 'tanh', 'activation_fn_key_pc_mode_3': 'tanh', 'activation_fn_key_pc_mode_4': 'tanh', 'activation_fn_key_pc_mode_5': 'gelu', 'input_dropout_key_pc_mode': 0.4040601897822085, 'pooler_dropout_key_pc_mode_0': 0.31670187825521173, 'pooler_dropout_key_pc_mode_1': 0.43573029509385885, 'pooler_dropout_key_pc_mode_2': 0.40183603844955723, 'pooler_dropout_key_pc_mode_3': 0.09328502944301792, 'pooler_dropout_key_pc_mode_4': 0.4462794992449889, 'pooler_dropout_key_pc_mode_5': 0.26967112095782536, 'normalisation_key_pc_mode_0': 'layer', 'normalisation_key_pc_mode_1': 'none', 'normalisation_key_pc_mode_2': 'layer', 'normalisation_key_pc_mode_3': 'layer', 'normalisation_key_pc_mode_4': 'layer', 'normalisation_key_pc_mode_5': 'none', 'num_linear_layers_primary_alteration_primary_degree_secondary_alteration_secondary_degree': 1, 'layer_dim_primary_alteration_primary_degree_secondary_alteration_secondary_degree_0': 280, 'layer_dim_primary_alteration_primary_degree_secondary_alteration_secondary_degree_1': 726, 'layer_dim_primary_alteration_primary_degree_secondary_alteration_secondary_degree_2': 270, 'layer_dim_primary_alteration_primary_degree_secondary_alteration_secondary_degree_3': 414, 'layer_dim_primary_alteration_primary_degree_secondary_alteration_secondary_degree_4': 550, 'layer_dim_primary_alteration_primary_degree_secondary_alteration_secondary_degree_5': 299, 'activation_fn_primary_alteration_primary_degree_secondary_alteration_secondary_degree_0': 'tanh', 'activation_fn_primary_alteration_primary_degree_secondary_alteration_secondary_degree_1': 'tanh', 'activation_fn_primary_alteration_primary_degree_secondary_alteration_secondary_degree_2': 'relu', 'activation_fn_primary_alteration_primary_degree_secondary_alteration_secondary_degree_3': 'gelu', 'activation_fn_primary_alteration_primary_degree_secondary_alteration_secondary_degree_4': 'gelu', 'activation_fn_primary_alteration_primary_degree_secondary_alteration_secondary_degree_5': 'tanh', 'input_dropout_primary_alteration_primary_degree_secondary_alteration_secondary_degree': 0.3808098076643588, 'pooler_dropout_primary_alteration_primary_degree_secondary_alteration_secondary_degree_0': 0.11881877199619983, 'pooler_dropout_primary_alteration_primary_degree_secondary_alteration_secondary_degree_1': 0.3641081743059298, 'pooler_dropout_primary_alteration_primary_degree_secondary_alteration_secondary_degree_2': 0.1838915663596266, 'pooler_dropout_primary_alteration_primary_degree_secondary_alteration_secondary_degree_3': 0.31615291529678974, 'pooler_dropout_primary_alteration_primary_degree_secondary_alteration_secondary_degree_4': 0.31676485538044735, 'pooler_dropout_primary_alteration_primary_degree_secondary_alteration_secondary_degree_5': 0.26788734203737924, 'normalisation_primary_alteration_primary_degree_secondary_alteration_secondary_degree_0': 'layer', 'normalisation_primary_alteration_primary_degree_secondary_alteration_secondary_degree_1': 'none', 'normalisation_primary_alteration_primary_degree_secondary_alteration_secondary_degree_2': 'layer', 'normalisation_primary_alteration_primary_degree_secondary_alteration_secondary_degree_3': 'none', 'normalisation_primary_alteration_primary_degree_secondary_alteration_secondary_degree_4': 'none', 'normalisation_primary_alteration_primary_degree_secondary_alteration_secondary_degree_5': 'none', 'freeze_layers': 8, 'learning_rate': 5.9356829120146846e-05, 'batch_size': 4}, user_attrs={}, system_attrs={}, intermediate_values={1000: 0.4154479576511704, 2000: 0.6685073408064323, 3000: 0.7300033691760366, 4000: 0.746977377981578, 5000: 0.7641094953659553, 6000: 0.7740767601089558, 7000: 0.7911810389625222, 8000: 0.8013242574964167, 9000: 0.8026301701148375, 10000: 0.80498787953197, 11000: 0.8046648811937162, 12000: 0.8056877687488937, 13000: 0.8141228435845749, 14000: 0.8164209499934331, 15000: 0.8139054889017057, 16000: 0.8164512868539319, 17000: 0.8210596344158476, 18000: 0.8207009456534773, 19000: 0.8235886578687391, 20000: 0.818115531330482, 21000: 0.8227677781140608, 22000: 0.8223441327797986, 23000: 0.8251433327432517, 24000: 0.8245216055551432, 25000: 0.8255327152703621, 26000: 0.824408466910694, 27000: 0.8280678060953535, 28000: 0.8248214051177214, 29000: 0.8284964480889919, 30000: 0.8286599102314453, 31000: 0.8279436034194282, 32000: 0.8273586373681596, 33000: 0.8286995266022146, 34000: 0.826214045466745, 35000: 0.8276127531878688, 36000: 0.8275510087541472, 37000: 0.8285749670220481, 38000: 0.8272729803502801}, distributions={'num_linear_layers_quality': IntDistribution(high=6, log=False, low=1, step=1), 'layer_dim_quality_0': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_quality_1': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_quality_2': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_quality_3': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_quality_4': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_quality_5': IntDistribution(high=768, log=False, low=32, step=1), 'activation_fn_quality_0': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_quality_1': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_quality_2': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_quality_3': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_quality_4': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_quality_5': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'input_dropout_quality': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_quality_0': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_quality_1': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_quality_2': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_quality_3': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_quality_4': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_quality_5': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'normalisation_quality_0': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_quality_1': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_quality_2': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_quality_3': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_quality_4': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_quality_5': CategoricalDistribution(choices=('none', 'layer')), 'num_linear_layers_inversion': IntDistribution(high=6, log=False, low=1, step=1), 'layer_dim_inversion_0': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_inversion_1': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_inversion_2': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_inversion_3': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_inversion_4': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_inversion_5': IntDistribution(high=768, log=False, low=32, step=1), 'activation_fn_inversion_0': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_inversion_1': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_inversion_2': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_inversion_3': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_inversion_4': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_inversion_5': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'input_dropout_inversion': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_inversion_0': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_inversion_1': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_inversion_2': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_inversion_3': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_inversion_4': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_inversion_5': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'normalisation_inversion_0': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_inversion_1': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_inversion_2': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_inversion_3': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_inversion_4': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_inversion_5': CategoricalDistribution(choices=('none', 'layer')), 'num_linear_layers_key_pc_mode': IntDistribution(high=6, log=False, low=1, step=1), 'layer_dim_key_pc_mode_0': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_key_pc_mode_1': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_key_pc_mode_2': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_key_pc_mode_3': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_key_pc_mode_4': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_key_pc_mode_5': IntDistribution(high=768, log=False, low=32, step=1), 'activation_fn_key_pc_mode_0': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_key_pc_mode_1': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_key_pc_mode_2': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_key_pc_mode_3': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_key_pc_mode_4': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_key_pc_mode_5': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'input_dropout_key_pc_mode': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_key_pc_mode_0': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_key_pc_mode_1': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_key_pc_mode_2': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_key_pc_mode_3': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_key_pc_mode_4': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_key_pc_mode_5': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'normalisation_key_pc_mode_0': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_key_pc_mode_1': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_key_pc_mode_2': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_key_pc_mode_3': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_key_pc_mode_4': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_key_pc_mode_5': CategoricalDistribution(choices=('none', 'layer')), 'num_linear_layers_primary_alteration_primary_degree_secondary_alteration_secondary_degree': IntDistribution(high=6, log=False, low=1, step=1), 'layer_dim_primary_alteration_primary_degree_secondary_alteration_secondary_degree_0': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_primary_alteration_primary_degree_secondary_alteration_secondary_degree_1': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_primary_alteration_primary_degree_secondary_alteration_secondary_degree_2': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_primary_alteration_primary_degree_secondary_alteration_secondary_degree_3': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_primary_alteration_primary_degree_secondary_alteration_secondary_degree_4': IntDistribution(high=768, log=False, low=32, step=1), 'layer_dim_primary_alteration_primary_degree_secondary_alteration_secondary_degree_5': IntDistribution(high=768, log=False, low=32, step=1), 'activation_fn_primary_alteration_primary_degree_secondary_alteration_secondary_degree_0': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_primary_alteration_primary_degree_secondary_alteration_secondary_degree_1': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_primary_alteration_primary_degree_secondary_alteration_secondary_degree_2': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_primary_alteration_primary_degree_secondary_alteration_secondary_degree_3': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_primary_alteration_primary_degree_secondary_alteration_secondary_degree_4': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'activation_fn_primary_alteration_primary_degree_secondary_alteration_secondary_degree_5': CategoricalDistribution(choices=('tanh', 'relu', 'gelu')), 'input_dropout_primary_alteration_primary_degree_secondary_alteration_secondary_degree': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_primary_alteration_primary_degree_secondary_alteration_secondary_degree_0': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_primary_alteration_primary_degree_secondary_alteration_secondary_degree_1': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_primary_alteration_primary_degree_secondary_alteration_secondary_degree_2': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_primary_alteration_primary_degree_secondary_alteration_secondary_degree_3': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_primary_alteration_primary_degree_secondary_alteration_secondary_degree_4': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'pooler_dropout_primary_alteration_primary_degree_secondary_alteration_secondary_degree_5': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'normalisation_primary_alteration_primary_degree_secondary_alteration_secondary_degree_0': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_primary_alteration_primary_degree_secondary_alteration_secondary_degree_1': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_primary_alteration_primary_degree_secondary_alteration_secondary_degree_2': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_primary_alteration_primary_degree_secondary_alteration_secondary_degree_3': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_primary_alteration_primary_degree_secondary_alteration_secondary_degree_4': CategoricalDistribution(choices=('none', 'layer')), 'normalisation_primary_alteration_primary_degree_secondary_alteration_secondary_degree_5': CategoricalDistribution(choices=('none', 'layer')), 'freeze_layers': IntDistribution(high=11, log=False, low=0, step=1), 'learning_rate': FloatDistribution(high=0.001, log=True, low=1e-05, step=None), 'batch_size': CategoricalDistribution(choices=(4, 8, 16, 32))}, trial_id=199, value=None)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "177c6b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " using model with the args Config(data_dir='/work/ui556004/data/rnbert/datasets/rnbert_seqs/data/', checkpoint_path='/hpcwork/ui556004/musicbert_hf/results/baseline_finetune_3/checkpoint-47000', targets=['quality', 'inversion', 'key_pc_mode', 'primary_alteration_primary_degree_secondary_alteration_secondary_degree'], output_dir_base='results_baseline', conditioning=None, log_dir='/home/ui556004/tmp/musicbert_hf_logs', num_epochs=0, batch_size=4, learning_rate=0.00025, warmup_steps=0, max_steps=-1, hf_repository=None, hf_token=None, DEBUG=True, RUN_NAS=False, num_trials=1, wandb_project=None, wandb_name=None, freeze_layers=None, job_id='1767522715', seed=42, name=None, optuna_storage=None, optuna_name=None, sampler_path=None, time_limit=None, baseline=True, trial_number=None, data_dir_for_metadata=None, msdebug=False, overwrite=False, compound_token_ratio=8, ignore_specials=4, task='musicbert_multitask_sequence_tagging', head='sequence_multitask_tagging_head', max_examples=None, dataset='test', ref_dir=None)\n",
      "Evaluating best model on test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='296' max='296' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [296/296 23:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ui556004/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ui556004/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpolinaitsme\u001b[0m (\u001b[33mpolinaitsme-RWTH Aachen University\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/rwthfs/rz/cluster/home/ui556004/projects/musicbert_hf/scripts/wandb/run-20260104_123133-d743p1yk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/polinaitsme-RWTH%20Aachen%20University/huggingface/runs/d743p1yk' target=\"_blank\">trainer_output</a></strong> to <a href='https://wandb.ai/polinaitsme-RWTH%20Aachen%20University/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/polinaitsme-RWTH%20Aachen%20University/huggingface' target=\"_blank\">https://wandb.ai/polinaitsme-RWTH%20Aachen%20University/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/polinaitsme-RWTH%20Aachen%20University/huggingface/runs/d743p1yk' target=\"_blank\">https://wandb.ai/polinaitsme-RWTH%20Aachen%20University/huggingface/runs/d743p1yk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss: 0.6170\n",
      "eval_model_preparation_time: 0.0023\n",
      "eval_quality_precision: 0.6606\n",
      "eval_quality_recall: 0.6173\n",
      "eval_quality_accuracy: 0.8653\n",
      "eval_inversion_precision: 0.8657\n",
      "eval_inversion_recall: 0.8464\n",
      "eval_inversion_accuracy: 0.8730\n",
      "eval_key_pc_mode_precision: 0.7935\n",
      "eval_key_pc_mode_recall: 0.8332\n",
      "eval_key_pc_mode_accuracy: 0.8152\n",
      "eval_primary_alteration_primary_degree_secondary_alteration_secondary_degree_precision: 0.1979\n",
      "eval_primary_alteration_primary_degree_secondary_alteration_secondary_degree_recall: 0.1710\n",
      "eval_primary_alteration_primary_degree_secondary_alteration_secondary_degree_accuracy: 0.7557\n",
      "eval_precision: 0.6294\n",
      "eval_recall: 0.6170\n",
      "eval_accuracy: 0.8273\n",
      "eval_runtime: 1404.6335\n",
      "eval_samples_per_second: 0.8410\n",
      "eval_steps_per_second: 0.2110\n"
     ]
    }
   ],
   "source": [
    "print(f\" using model with the args {args}\")\n",
    "print(\"Evaluating best model on test set...\")\n",
    "test_results = test_trainer.evaluate()\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2c41116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " using model with the args Config(data_dir='/work/ui556004/data/rnbert/datasets/rnbert_seqs/data/', checkpoint_path='/hpcwork/ui556004/results/musicbert_hf/checkpoints/new_nas/trial_0000/checkpoint-33000', targets=['quality', 'inversion', 'key_pc_mode', 'primary_alteration_primary_degree_secondary_alteration_secondary_degree'], output_dir_base='0results_hpo', conditioning=None, log_dir='/home/ui556004/tmp/musicbert_hf_logs', num_epochs=0, batch_size=4, learning_rate=0.00025, warmup_steps=0, max_steps=-1, hf_repository=None, hf_token=None, DEBUG=True, RUN_NAS=False, num_trials=1, wandb_project=None, wandb_name=None, freeze_layers=None, job_id='1767629219', seed=42, name=None, optuna_storage='sqlite:///optuna_nas.db', optuna_name='new_nas', sampler_path=None, time_limit=None, baseline=False, trial_number=0, data_dir_for_metadata=None, msdebug=False, overwrite=False, compound_token_ratio=8, ignore_specials=4, task='musicbert_multitask_sequence_tagging', head='sequence_multitask_tagging_head', max_examples=None, dataset='test', ref_dir=None)\n",
      "Evaluating best model on test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='296' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 33/296 02:34 < 21:10, 0.21 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m using model with the args \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating best model on test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mtest_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m test_results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/transformers/trainer.py:4105\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4102\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4104\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4105\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4106\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   4109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   4110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4115\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/transformers/trainer.py:4299\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4296\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   4298\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 4299\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4300\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4301\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4303\u001b[0m )\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/transformers/trainer.py:4515\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   4513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[1;32m   4514\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4515\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   4516\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m   4518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/transformers/trainer.py:3759\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3757\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3758\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3759\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3760\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3761\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/rwthfs/rz/cluster/home/ui556004/projects/musicbert_hf/musicbert_hf/models.py:716\u001b[0m, in \u001b[0;36mMusicBertMultiTaskTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m label\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m, (\n\u001b[1;32m    713\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels must have shape (batch_size, sequence_length)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    714\u001b[0m         )\n\u001b[0;32m--> 716\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    730\u001b[0m \u001b[38;5;66;03m# TODO: (Malcolm 2024-03-16) do we want to add dropout here?\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;66;03m# sequence_output = self.dropout(sequence_output)\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/rwthfs/rz/cluster/home/ui556004/projects/musicbert_hf/musicbert_hf/models.py:179\u001b[0m, in \u001b[0;36mMusicBertEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(((batch_size, seq)), device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 179\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "File \u001b[0;32m/rwthfs/rz/cluster/home/ui556004/projects/musicbert_hf/musicbert_hf/hf_monkeypatch.py:190\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    188\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 190\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    203\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/transformers/pytorch_utils.py:261\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/transformers/models/bert/modeling_bert.py:640\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/transformers/models/bert/modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 552\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    554\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/projects/musicbert_hf/.venv/lib64/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\" using model with the args {args}\")\n",
    "print(\"Evaluating best model on test set...\")\n",
    "test_results = test_trainer.evaluate()\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50166ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functools.partial(<function compute_metrics_multitask at 0x15370e9e4c20>, task_names=['quality', 'inversion', 'key_pc_mode', 'primary_alteration_primary_degree_secondary_alteration_secondary_degree'])\n"
     ]
    }
   ],
   "source": [
    "print(compute_metrics_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "512996ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function compute_metrics_multitask at 0x15370e9e4c20>\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "if isinstance(compute_metrics_fn, partial):\n",
    "    real_fn = compute_metrics_fn.func\n",
    "else:\n",
    "    real_fn = compute_metrics_fn\n",
    "\n",
    "print(real_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a23e94f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined in: /rwthfs/rz/cluster/home/ui556004/projects/musicbert_hf/musicbert_hf/metrics.py\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(\"Defined in:\", inspect.getsourcefile(real_fn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
